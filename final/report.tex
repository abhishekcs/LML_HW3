\documentclass{article}

\usepackage[a4paper]{geometry}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{times}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{hyperref}
\usepackage{url}
\usepackage{mathtools}
\usepackage{graphicx}
\usepackage{algorithm,algorithmic}
\usepackage{natbib}
\begin{document}
\title{Large Scale Machine Learning - Assignment 3\\Abhishek Sinha, Arun Sai, Ashish Bora}
\maketitle
\section{Q1}
\begin{itemize}
\item Model:  $\ell$1 regularized logsitic regression
\item Private Score: 0.896
\end{itemize}

For this question we generated additional features from the original categorical features. All pairs and triples of the original categorical features were generated. Also used one-hot encoding.
\section{Q2: XGBoost}
\begin{itemize}
\item Model:  Boosted Decision Trees. 
\item Private Score: 0.883
\end{itemize}

All pairs and triples of the original categorical features and their frequencies (i.e, number of times a particular pair or a triple occurred) were used to train the model. Doesn't use one-hot encoding. Best parameters found using cross validation: $learning\_rate =0.2, n\_estimators=100, colsample\_bytree = 0.1, max\_depth=6$


\section{Q2: XGBoost with one-hot encoding}
\section{Q3}
\begin{itemize}
\item Model:  ensemble of $\ell$1 regularized logsitic regression, XGBoost with one-hot encoding, XGBoost without one-hot encoding and random forests trained with 'entropy' criterion.
\item Private Score: 0.9079
\end{itemize}
\textbf{Best parameters for Random Forest}\\
The Random Forest was trained on the original features. One hot-encoding was not used for categorical features. The best parameters obtained using 5-fold cross validation were-- $n\_estimators= 270, max\_features=4, max\_depth=23, min\_samples\_leaf=2,min\_samples\_split=8, criterion=entropy$. The private score only with Random Forest was 0.8762.
\end{document}
