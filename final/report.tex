\documentclass{article}

\usepackage[a4paper]{geometry}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{times}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{hyperref}
\usepackage{url}
\usepackage{mathtools}
\usepackage{graphicx}
\usepackage{algorithm,algorithmic}
\usepackage{natbib}
\begin{document}
\title{Large Scale Machine Learning - Assignment 3\\Abhishek Sinha, Arun Sai, Ashish Bora}
\maketitle
\section{Q1}
\begin{itemize}
\item Model:  $\ell$1 regularized logsitic regression
\item Private Score: 0.896
\end{itemize}

For this question we generated additional features from the original categorical features. All pairs and triples of the original categorical features were generated. Also used one-hot encoding.
\section{Q2: XGBoost}
\begin{itemize}
\item Model:  Boosted Decision Trees. 
\item Private Score: 0.883
\end{itemize}

All pairs and triples of the original categorical features and their frequencies (i.e, number of times a particular pair or a triple occurred) were used to train the model. Doesn't use one-hot encoding. Best parameters found using cross validation: $learning\_rate =0.2, n\_estimators=100, colsample\_bytree = 0.1, max\_depth=6$


\section{Q2: XGBoost with one-hot encoding}
\section{Q3}
\textit{Random Forests..}\\
\textit{Ensemble..}
\end{document}
